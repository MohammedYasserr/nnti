{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10853574,"sourceType":"datasetVersion","datasetId":6741275}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries for the implementation\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport torch.nn as nn \nimport torch.autograd as autograd\nfrom torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\nfrom datasets import load_dataset\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nimport random\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:18:54.047352Z","iopub.execute_input":"2025-02-27T13:18:54.047649Z","iopub.status.idle":"2025-02-27T13:19:02.827171Z","shell.execute_reply.started":"2025-02-27T13:18:54.047624Z","shell.execute_reply":"2025-02-27T13:19:02.826311Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Define paths and constants\nDATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"  # Path to the original dataset\nMODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"  # Pre-trained model name\nBATCH_SIZE = 16  # Batch size for data loading\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Device for computation\nSEED = 42  # Random seed for reproducibility\nLR = 2e-5  # Learning rate for optimization\nWEIGHT_DECAY = 0.01  # Weight decay for regularization\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nset_seed(SEED)\n\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:19:16.069678Z","iopub.execute_input":"2025-02-27T13:19:16.069998Z","iopub.status.idle":"2025-02-27T13:19:16.128311Z","shell.execute_reply.started":"2025-02-27T13:19:16.069971Z","shell.execute_reply":"2025-02-27T13:19:16.127502Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# This cell handles loading the external dataset that contains additional \n# molecules that might improve model performance\n\n# Try to load the external dataset from the expected Kaggle path\ntry:\n    # Attempt to load from the Kaggle path mentioned\n    ext_data_path = \"../input/external-smiles/External-Dataset_for_Task2.csv\"\n    if os.path.exists(ext_data_path):\n        ext_data = pd.read_csv(ext_data_path)\n    else:\n        # Try alternative paths\n        alt_paths = [\n            \"External-Dataset-for-Task2.csv\",\n            \"../tasks/External-Dataset_for_Task2.csv\",\n            \"/kaggle/input/external-smiles/External-Dataset-for-Task2.csv\"\n        ]\n        \n        for path in alt_paths:\n            if os.path.exists(path):\n                ext_data = pd.read_csv(path)\n                print(f\"Loaded external dataset from {path}\")\n                break\n        else:\n            raise FileNotFoundError(\"External dataset file not found\")\n            \nexcept Exception as e:\n    print(f\"Error loading external dataset: {e}\")\n    # Create a dummy dataset if needed for testing\n    print(\"Creating a dummy external dataset for demonstration\")\n    ext_data = pd.DataFrame({\n        'SMILES': [\"CCC\", \"CCO\", \"CCN\", \"CC=O\"],\n        'Label': [1.2, 0.8, 1.5, 0.6]\n    })\n\n# Display information about the external dataset\nprint(f\"External dataset loaded with {len(ext_data)} samples\")\nprint(\"First few rows of the external dataset:\")\ndisplay(ext_data.head())\n\n# Check for missing values\nprint(\"\\nMissing values in the external dataset:\")\nprint(ext_data.isnull().sum())\n\n# Show basic statistics\nprint(\"\\nBasic statistics of the external dataset:\")\nprint(ext_data.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:19:23.188898Z","iopub.execute_input":"2025-02-27T13:19:23.189190Z","iopub.status.idle":"2025-02-27T13:19:23.236264Z","shell.execute_reply.started":"2025-02-27T13:19:23.189168Z","shell.execute_reply":"2025-02-27T13:19:23.235451Z"}},"outputs":[{"name":"stdout","text":"External dataset loaded with 300 samples\nFirst few rows of the external dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                              SMILES  Label\n0              CCCCN1Cc2c(nc3cc(-c4ccco4)nn3c2O)C1=O  1.548\n1               Cc1cc(C)c2c(n1)sc1c2ncnc1N1CCN(C)CC1  2.568\n2  COC(=O)[C@H]1[C@H]2CC[C@H](C[C@@H]1OC(=O)c1ccc...  0.102\n3                   Nc1nonc1/C(=N/O)Nc1ccc(F)c(Cl)c1  2.450\n4         Cc1c[nH]c(/C=C2/C(=O)Nc3ccccc32)c1CCC(=O)O  1.040","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SMILES</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CCCCN1Cc2c(nc3cc(-c4ccco4)nn3c2O)C1=O</td>\n      <td>1.548</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cc1cc(C)c2c(n1)sc1c2ncnc1N1CCN(C)CC1</td>\n      <td>2.568</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>COC(=O)[C@H]1[C@H]2CC[C@H](C[C@@H]1OC(=O)c1ccc...</td>\n      <td>0.102</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Nc1nonc1/C(=N/O)Nc1ccc(F)c(Cl)c1</td>\n      <td>2.450</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cc1c[nH]c(/C=C2/C(=O)Nc3ccccc32)c1CCC(=O)O</td>\n      <td>1.040</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nMissing values in the external dataset:\nSMILES    0\nLabel     0\ndtype: int64\n\nBasic statistics of the external dataset:\n            Label\ncount  300.000000\nmean     1.790927\nstd      0.878076\nmin     -0.338000\n25%      1.242500\n50%      1.898000\n75%      2.372500\nmax      4.028000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# This cell defines classes for the model architecture and datasets\n\nclass MoLFormerWithRegressionHead(nn.Module):\n    def __init__(self, base_model, dropout_rate=0.1):\n        \"\"\"\n        Model architecture that combines MoLFormer with a regression head.\n        \n        Args:\n            base_model: Pre-trained MoLFormer model\n            dropout_rate: Dropout probability for regularization\n        \"\"\"\n        super().__init__()\n        \n        # Base MoLFormer model\n        self.base_model = base_model\n        \n        # Get the hidden size from the base model config\n        hidden_size = base_model.config.hidden_size\n        \n        # Regression head with dropout for regularization\n        self.regression_head = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size // 2, 1)\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Forward pass of the model.\n        \n        Args:\n            input_ids: Tokenized input sequences\n            attention_mask: Attention mask for the sequences\n            \n        Returns:\n            Predicted values (logits)\n        \"\"\"\n        # Pass through base model\n        outputs = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use the [CLS] token representation for regression\n        sequence_output = outputs.last_hidden_state[:, 0, :]\n        \n        # Pass through regression head and remove extra dimension\n        logits = self.regression_head(sequence_output)\n        \n        return logits.squeeze(-1)  # Remove last dimension for single value prediction\n\n\nclass SMILESDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        \"\"\"\n        Dataset for SMILES strings with lipophilicity labels.\n        \n        Args:\n            data: Dataset containing SMILES strings and labels\n            tokenizer: Tokenizer for processing SMILES strings\n            max_length: Maximum length for padding/truncation\n        \"\"\"\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Get a single sample from the dataset.\n        \n        Args:\n            idx: Index of the sample to retrieve\n            \n        Returns:\n            Dictionary containing input_ids, attention_mask, and labels\n        \"\"\"\n        # Handle Subset indexing\n        if isinstance(self.data, Subset):\n            item = self.data.dataset[self.data.indices[idx]]\n        else:\n            item = self.data[idx]\n        \n        # Get the SMILE and target\n        smiles = item['SMILES']\n        target = item['Label'] if 'Label' in item else item['label']\n        \n        # Tokenize the SMILES string\n        encoding = self.tokenizer(\n            smiles,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Remove the batch dimension added by the tokenizer\n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # Convert target to tensor\n        target = torch.tensor(target, dtype=torch.float32)\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': target\n        }\n\n\nclass ExternalSMILESDataset(Dataset):\n    \"\"\"\n    Dataset class specifically for the external dataset.\n    Similar to SMILESDataset but handles the pandas DataFrame format.\n    \"\"\"\n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        # Get the SMILES string and label from the DataFrame\n        smiles = self.dataframe.iloc[idx]['SMILES']\n        label = self.dataframe.iloc[idx]['Label']\n        \n        # Tokenize the SMILES string\n        encoding = self.tokenizer(\n            smiles,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Remove the batch dimension\n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # Convert label to tensor\n        label = torch.tensor(label, dtype=torch.float32)\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': label,\n            'idx': idx  # Include the index for tracking\n        }\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:19:30.231508Z","iopub.execute_input":"2025-02-27T13:19:30.231798Z","iopub.status.idle":"2025-02-27T13:19:30.242505Z","shell.execute_reply.started":"2025-02-27T13:19:30.231777Z","shell.execute_reply":"2025-02-27T13:19:30.241650Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# This cell loads the original dataset (Lipophilicity) and the pre-trained model\n\n# Load tokenizer for processing SMILES strings\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# Load original dataset\ndataset = load_dataset(DATASET_PATH)\nprint(f\"Dataset loaded with {len(dataset['train'])} samples\")\n\n# Split the dataset into training and test sets\ndef split_dataset(dataset, tokenizer, test_size=0.2, random_state=42):\n    \"\"\"\n    Split a dataset into training and test sets.\n    \n    Args:\n        dataset: HuggingFace dataset to split\n        tokenizer: Tokenizer for processing SMILES strings\n        test_size: Proportion of dataset to use for testing\n        random_state: Random seed for reproducibility\n    \n    Returns:\n        train_data: SMILESDataset for training\n        test_data: SMILESDataset for testing\n    \"\"\"\n    # Set random seeds for reproducibility\n    set_seed(random_state)\n    \n    # Get total number of samples\n    total_samples = len(dataset['train'])\n    \n    # Generate random indices for splitting\n    indices = list(range(total_samples))\n    random.shuffle(indices)\n    \n    # Calculate split point\n    split_idx = int(total_samples * (1 - test_size))\n    \n    # Split indices into train and test\n    train_indices = indices[:split_idx]\n    test_indices = indices[split_idx:]\n    \n    # Create Subset datasets\n    train_dataset = Subset(dataset['train'], train_indices)\n    test_dataset = Subset(dataset['train'], test_indices)\n    \n    # Create SMILES datasets with the tokenizer\n    train_data = SMILESDataset(train_dataset, tokenizer)\n    test_data = SMILESDataset(test_dataset, tokenizer)\n    \n    # Print dataset sizes\n    print(f\"Total dataset size: {total_samples}\")\n    print(f\"Training set size: {len(train_data)}\")\n    print(f\"Test set size: {len(test_data)}\")\n    \n    return train_data, test_data\n\n# Split the dataset\ntrain_data, test_data = split_dataset(dataset, tokenizer)\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_data, \n    batch_size=BATCH_SIZE, \n    shuffle=True\n)\n\ntest_loader = DataLoader(\n    test_data, \n    batch_size=BATCH_SIZE, \n    shuffle=False\n)\n\n# Create external dataset loader\nexternal_dataset = ExternalSMILESDataset(ext_data, tokenizer)\nexternal_loader = DataLoader(\n    external_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False  # No shuffling to maintain index correspondence\n)\n\nprint(f\"External dataset size: {len(external_dataset)}\")\n\n# Load pre-trained model\nprint(\"Loading pre-trained MoLFormer model...\")\nbase_model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\nmodel = MoLFormerWithRegressionHead(base_model).to(DEVICE)\n\n# Load or train the model (we'll need the pre-trained weights from Task 1)\ntry:\n    # First try to load the best model from Task 1\n    model.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))\n    print(\"Successfully loaded pre-trained model from Task 1\")\nexcept FileNotFoundError:\n    print(\"Pre-trained model not found, will need to train from scratch\")\n    # We would train the model here if needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:19:36.505186Z","iopub.execute_input":"2025-02-27T13:19:36.505527Z","iopub.status.idle":"2025-02-27T13:19:55.676173Z","shell.execute_reply.started":"2025-02-27T13:19:36.505502Z","shell.execute_reply":"2025-02-27T13:19:55.675218Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9667b22a5b3c495bbba6b5d0c837aef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_molformer_fast.py:   0%|          | 0.00/6.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b39501b1a464d969cc6fe23f4f19ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_molformer.py:   0%|          | 0.00/9.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cb2e8235fa344799e7ea4a5f3fa686c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- tokenization_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- tokenization_molformer_fast.py\n- tokenization_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4ee007fe8345e0a5f06591d41e6681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/54.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f194efb2c06346588cf2e1767a64954e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b07aaa7bf484f82a9e5a4e2d9ad2f61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d586dd30c7bf4244beeb0ec2a587d59b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"lipophilicity.csv:   0%|          | 0.00/223k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a130832f8424ed2b97606e6ed47568f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79a5823fbe7c401fb5fea7f08bfb69ec"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded with 4200 samples\nTotal dataset size: 4200\nTraining set size: 3360\nTest set size: 840\nExternal dataset size: 300\nLoading pre-trained MoLFormer model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a266415bd3a48d7b62e2f5d3e3e8915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_molformer.py:   0%|          | 0.00/7.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043817185e5e40fe81ad81ff8aa79aa3"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- configuration_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_molformer.py:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ea534829cd4f01a1ce38f68973e07c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- modeling_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/187M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"101d4ed0a8f34ba2900923d78cb851b9"}},"metadata":{}},{"name":"stdout","text":"Pre-trained model not found, will need to train from scratch\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-e22d56ed3c25>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# This cell contains training functions in case we need to train a model from scratch\n# Usually we'd load the pre-trained model from Task 1, but this is a fallback\n\ndef train_model(model, train_loader, test_loader, epochs=10, patience=3):\n    \"\"\"\n    Train the model with early stopping.\n    \n    Args:\n        model: Model to train\n        train_loader: DataLoader for training data\n        test_loader: DataLoader for validation/test data\n        epochs: Maximum number of training epochs\n        patience: Early stopping patience\n        \n    Returns:\n        Trained model and best validation loss\n    \"\"\"\n    # Initialize optimizer and loss function\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=LR,\n        weight_decay=WEIGHT_DECAY\n    )\n    criterion = nn.MSELoss()\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=2, verbose=True\n    )\n    \n    # Early stopping variables\n    best_val_loss = float('inf')\n    epochs_without_improvement = 0\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batch in progress_bar:\n            # Get batch data\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n            # Update running loss\n            train_loss += loss.item()\n            progress_bar.set_postfix({\"Loss\": loss.item()})\n            \n        # Calculate average training loss\n        avg_train_loss = train_loss / len(train_loader)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for batch in test_loader:\n                input_ids = batch['input_ids'].to(DEVICE)\n                attention_mask = batch['attention_mask'].to(DEVICE)\n                labels = batch['labels'].to(DEVICE)\n                \n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n        avg_val_loss = val_loss / len(test_loader)\n        \n        # Print epoch results\n        print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n        \n        # Update learning rate\n        scheduler.step(avg_val_loss)\n        \n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            epochs_without_improvement = 0\n            torch.save(model.state_dict(), \"new_best_model.pth\")\n            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n        else:\n            epochs_without_improvement += 1\n            if epochs_without_improvement >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                break\n                \n    # Load best model weights\n    model.load_state_dict(torch.load(\"new_best_model.pth\", map_location=DEVICE))\n    \n    return model, best_val_loss\n\n# Only train if no pre-trained model was found\nif not os.path.exists(\"best_model.pth\"):\n    print(\"No pre-trained model found. Training from scratch...\")\n    model, best_val_loss = train_model(model, train_loader, test_loader)\n    print(f\"Model trained with best validation loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:19:55.677214Z","iopub.execute_input":"2025-02-27T13:19:55.677735Z","iopub.status.idle":"2025-02-27T13:36:11.675506Z","shell.execute_reply.started":"2025-02-27T13:19:55.677712Z","shell.execute_reply":"2025-02-27T13:36:11.674596Z"}},"outputs":[{"name":"stdout","text":"No pre-trained model found. Training from scratch...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce42be5ec18e444da4ac6f435a27a2b1"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Training Loss: 1.2712, Validation Loss: 0.8642\nNew best model saved with validation loss: 0.8642\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf8029cf72d4e2c9b4c1bd97cd7f07b"}},"metadata":{}},{"name":"stdout","text":"Epoch 2, Training Loss: 0.6696, Validation Loss: 0.7608\nNew best model saved with validation loss: 0.7608\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cf3afe733d2457586ecd44b8b292784"}},"metadata":{}},{"name":"stdout","text":"Epoch 3, Training Loss: 0.4766, Validation Loss: 0.5616\nNew best model saved with validation loss: 0.5616\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6a6205b66e4835a67c6a2c35f9300b"}},"metadata":{}},{"name":"stdout","text":"Epoch 4, Training Loss: 0.3808, Validation Loss: 0.5542\nNew best model saved with validation loss: 0.5542\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d00ff73d4044bb880dd64584efff105"}},"metadata":{}},{"name":"stdout","text":"Epoch 5, Training Loss: 0.3345, Validation Loss: 0.5350\nNew best model saved with validation loss: 0.5350\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feff996869364883b9c6b76fb1addfc0"}},"metadata":{}},{"name":"stdout","text":"Epoch 6, Training Loss: 0.2750, Validation Loss: 0.4954\nNew best model saved with validation loss: 0.4954\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f227789c250143628ee695238ba46d8b"}},"metadata":{}},{"name":"stdout","text":"Epoch 7, Training Loss: 0.2403, Validation Loss: 0.4639\nNew best model saved with validation loss: 0.4639\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c289265d9a433989ba5397a6867e69"}},"metadata":{}},{"name":"stdout","text":"Epoch 8, Training Loss: 0.2082, Validation Loss: 0.4713\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b57a73c9634180bceb3fedd94713c5"}},"metadata":{}},{"name":"stdout","text":"Epoch 9, Training Loss: 0.1970, Validation Loss: 0.4969\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/210 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2334fbdc8349492180ddccb27b175c08"}},"metadata":{}},{"name":"stdout","text":"Epoch 10, Training Loss: 0.1820, Validation Loss: 0.4762\nEarly stopping triggered after 10 epochs\nModel trained with best validation loss: 0.4639\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-0d9e3a482ed7>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"new_best_model.pth\", map_location=DEVICE))\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# This cell contains the function to evaluate the model\n# We'll use this to compare performance before and after adding influential samples\n\ndef evaluate_model(model, test_loader):\n    \"\"\"\n    Evaluate model performance on the test set.\n    \n    Args:\n        model: Model to evaluate\n        test_loader: DataLoader for test data\n        \n    Returns:\n        Dictionary of evaluation metrics\n    \"\"\"\n    model.eval()\n    criterion = nn.MSELoss()\n    \n    # Tracking variables\n    test_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    # Evaluate without gradients\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n            # Get batch data\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask)\n            \n            # Compute loss\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            \n            # Store predictions and labels\n            all_preds.extend(outputs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    \n    # Calculate metrics\n    mse = np.mean((all_preds - all_labels) ** 2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(all_preds - all_labels))\n    \n    # Calculate R-squared\n    y_mean = np.mean(all_labels)\n    ss_total = np.sum((all_labels - y_mean) ** 2)\n    ss_residual = np.sum((all_labels - all_preds) ** 2)\n    r2 = 1 - (ss_residual / ss_total)\n    \n    # Average test loss\n    avg_loss = test_loss / len(test_loader)\n    \n    # Print results\n    print(\"\\nTest Results:\")\n    print(f\"Average Loss: {avg_loss:.4f}\")\n    print(f\"MSE: {mse:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n    print(f\"R²: {r2:.4f}\")\n    \n    # Return metrics as dictionary\n    metrics = {\n        'loss': avg_loss,\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'r2': r2\n    }\n    \n    return metrics\n\n# Evaluate the baseline model (from Task 1)\nprint(\"Evaluating baseline model from Task 1...\")\nbaseline_metrics = evaluate_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:42:05.789677Z","iopub.execute_input":"2025-02-27T13:42:05.789995Z","iopub.status.idle":"2025-02-27T13:42:14.261341Z","shell.execute_reply.started":"2025-02-27T13:42:05.789971Z","shell.execute_reply":"2025-02-27T13:42:14.260678Z"}},"outputs":[{"name":"stdout","text":"Evaluating baseline model from Task 1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating model:   0%|          | 0/53 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c132d16532864e70b84b14c7ae0d27e3"}},"metadata":{}},{"name":"stdout","text":"\nTest Results:\nAverage Loss: 0.4706\nMSE: 0.4735\nRMSE: 0.6881\nMAE: 0.5384\nR²: 0.7028\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Define functions for computing influence scores\n\ndef get_loss_grad(model, batch, criterion):\n    \"\"\"\n    Compute the gradient of the loss with respect to model parameters for a single batch.\n    \n    Args:\n        model: The neural network model\n        batch: A batch of data\n        criterion: Loss function\n        \n    Returns:\n        Gradient vectors flattened and concatenated\n    \"\"\"\n    # Get batch data\n    input_ids = batch['input_ids'].to(DEVICE)\n    attention_mask = batch['attention_mask'].to(DEVICE)\n    labels = batch['labels'].to(DEVICE)\n    \n    # Forward pass\n    model.zero_grad()\n    outputs = model(input_ids, attention_mask)\n    loss = criterion(outputs, labels)\n    \n    # Compute gradients\n    loss.backward()\n    \n    # Collect and flatten gradients\n    grad_vec = []\n    for param in model.parameters():\n        if param.requires_grad and param.grad is not None:\n            grad_vec.append(param.grad.detach().view(-1))\n    \n    # Concatenate all gradients into a single vector\n    grad_vec = torch.cat(grad_vec)\n    \n    return grad_vec\n\ndef hvp(model, batch, v, criterion, damping=1e-3):\n    \"\"\"\n    Compute the Hessian-vector product.\n    \n    Args:\n        model: The neural network model\n        batch: A batch of data\n        v: Vector to compute product with\n        criterion: Loss function\n        damping: Damping factor for numerical stability\n        \n    Returns:\n        Hessian-vector product\n    \"\"\"\n    # Get batch data\n    input_ids = batch['input_ids'].to(DEVICE)\n    attention_mask = batch['attention_mask'].to(DEVICE)\n    labels = batch['labels'].to(DEVICE)\n    \n    # Forward pass\n    model.zero_grad()\n    outputs = model(input_ids, attention_mask)\n    loss = criterion(outputs, labels)\n    \n    # Compute first-order gradients\n    grads = autograd.grad(loss, model.parameters(), create_graph=True)\n    \n    # Flatten gradients\n    grads_flat = torch.cat([g.view(-1) for g in grads])\n    \n    # Compute gradient-vector product\n    grad_v_prod = torch.sum(grads_flat * v)\n    \n    # Compute Hessian-vector product\n    hvp_vec = autograd.grad(grad_v_prod, model.parameters(), retain_graph=True)\n    \n    # Flatten and concatenate\n    hvp_flat = torch.cat([g.detach().view(-1) for g in hvp_vec])\n    \n    # Add damping for numerical stability\n    return hvp_flat + damping * v\n\ndef lissa_ihvp(model, v, train_loader, criterion, test_batch=None, recursion_depth=10, damping=0.01):\n    \"\"\"\n    Improved LiSSA approximation with better numerical stability.\n    \n    Args:\n        model: The neural network model\n        v: Vector to compute product with\n        train_loader: DataLoader for training data\n        criterion: Loss function \n        test_batch: Optional test batch (not used in this implementation)\n        recursion_depth: Number of recursive approximations\n        damping: Damping factor for numerical stability\n        \n    Returns:\n        Approximated inverse-Hessian-vector product\n    \"\"\"\n    # Initialize estimate to zeros with same shape as v\n    ihvp_estimate = torch.zeros_like(v)\n    \n    # Normalize the input vector for stability\n    v_norm = torch.norm(v)\n    if v_norm > 0:\n        v = v / v_norm\n    \n    # Perform recursive approximation with better numerical stability\n    for i in range(recursion_depth):\n        # Sample a random batch\n        try:\n            train_iter = iter(train_loader)\n            sampled_batch = next(train_iter)\n        except StopIteration:\n            # Reset iterator if needed\n            train_iter = iter(train_loader)\n            sampled_batch = next(train_iter)\n        \n        # Update estimate: v + (I - HVP) * prev_estimate\n        hvp_val = hvp(model, sampled_batch, ihvp_estimate if i > 0 else v, criterion, damping)\n        ihvp_estimate = v + ihvp_estimate - hvp_val\n        \n        # Apply scaling to prevent overflow\n        scale = min(1.0, 10.0 / (torch.norm(ihvp_estimate) + 1e-10))\n        ihvp_estimate = ihvp_estimate * scale\n        \n        if (i+1) % 5 == 0:\n            print(f\"LiSSA iteration {i+1}/{recursion_depth} completed\")\n    \n    # Rescale back if we normalized v\n    if v_norm > 0:\n        ihvp_estimate = ihvp_estimate * v_norm\n    \n    return ihvp_estimate\n\ndef compute_influence_score(model, train_batch, test_grad, ihvp_estimate, criterion):\n    \"\"\"\n    Compute influence with proper scaling.\n    \"\"\"\n    # Compute gradient of training loss\n    train_grad = get_loss_grad(model, train_batch, criterion)\n    \n    # Normalize gradients for stability\n    train_grad_norm = torch.norm(train_grad)\n    \n    # Compute dot product with proper scaling\n    if train_grad_norm > 0:\n        train_grad = train_grad / train_grad_norm\n    \n    # Compute influence: -test_grad * ihvp * train_grad\n    influence = -torch.dot(test_grad, ihvp_estimate) * train_grad\n    \n    # Return scalar influence score (average over all dimensions)\n    return float(influence.mean().item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:42:20.273052Z","iopub.execute_input":"2025-02-27T13:42:20.273338Z","iopub.status.idle":"2025-02-27T13:42:20.285193Z","shell.execute_reply.started":"2025-02-27T13:42:20.273318Z","shell.execute_reply":"2025-02-27T13:42:20.284218Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def compute_external_influence_scores(model, external_loader, train_loader, test_loader, num_samples=50):\n    \"\"\"\n    Compute influence scores using a simplified approach.\n    \"\"\"\n    model.eval()\n    criterion = nn.MSELoss()\n    \n    # Store results\n    influence_scores = []\n    \n    # Select a small set of test samples for influence computation\n    test_samples = []\n    for batch in test_loader:\n        for i in range(min(batch['input_ids'].size(0), 5)):  # Take 5 samples from each batch\n            sample = {\n                'input_ids': batch['input_ids'][i:i+1].to(DEVICE),\n                'attention_mask': batch['attention_mask'][i:i+1].to(DEVICE),\n                'labels': batch['labels'][i:i+1].to(DEVICE)\n            }\n            test_samples.append(sample)\n        if len(test_samples) >= num_samples:\n            test_samples = test_samples[:num_samples]\n            break\n    \n    print(f\"Using {len(test_samples)} test samples for influence computation\")\n    \n    # Compute influence for each external sample\n    print(\"Computing influence scores...\")\n    for batch_idx, batch in enumerate(tqdm(external_loader)):\n        batch_influence = []\n        \n        for i in range(batch['input_ids'].size(0)):\n            # Extract single sample\n            ext_sample = {\n                'input_ids': batch['input_ids'][i:i+1].to(DEVICE),\n                'attention_mask': batch['attention_mask'][i:i+1].to(DEVICE),\n                'labels': batch['labels'][i:i+1].to(DEVICE),\n            }\n            \n            idx = batch['idx'][i].item()\n            \n            # Compute similarity-based influence across test samples\n            sample_influence = 0\n            \n            # Get external sample gradient\n            ext_grad = get_loss_grad(model, ext_sample, criterion)\n            ext_grad_norm = torch.norm(ext_grad)\n            if ext_grad_norm > 0:\n                ext_grad = ext_grad / ext_grad_norm\n                \n            # Compute influence by dot product with test gradients\n            for test_sample in test_samples:\n                # Get test sample gradient\n                test_grad = get_loss_grad(model, test_sample, criterion)\n                test_grad_norm = torch.norm(test_grad)\n                \n                if test_grad_norm > 0:\n                    test_grad = test_grad / test_grad_norm\n                    \n                    # Compute similarity (dot product of normalized gradients)\n                    # Negative because we want samples that point in opposite direction\n                    # to reduce the test loss\n                    similarity = -torch.dot(ext_grad, test_grad).item()\n                    sample_influence += similarity\n            \n            # Average over all test samples\n            sample_influence /= len(test_samples)\n            \n            influence_scores.append({\n                'idx': idx,\n                'influence_score': sample_influence,\n                'smiles': ext_data.iloc[idx]['SMILES'],\n                'label': ext_data.iloc[idx]['Label']\n            })\n    \n    # Convert to DataFrame and sort\n    results_df = pd.DataFrame(influence_scores)\n    results_df = results_df.sort_values('influence_score', ascending=False).reset_index(drop=True)\n    \n    return results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:42:24.107093Z","iopub.execute_input":"2025-02-27T13:42:24.107422Z","iopub.status.idle":"2025-02-27T13:42:24.115886Z","shell.execute_reply.started":"2025-02-27T13:42:24.107369Z","shell.execute_reply":"2025-02-27T13:42:24.115183Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def select_influential_samples(influence_df, top_k=100):\n    \"\"\"\n    Select the most influential samples based on influence scores.\n    \n    Args:\n        influence_df: DataFrame with influence scores\n        top_k: Number of top samples to select\n        \n    Returns:\n        DataFrame with selected samples\n    \"\"\"\n    # Select top-k samples with highest influence scores\n    selected = influence_df.head(top_k)\n    \n    print(f\"Selected {len(selected)} samples with influence scores ranging from {selected['influence_score'].min():.4f} to {selected['influence_score'].max():.4f}\")\n    \n    return selected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:38:20.056882Z","iopub.execute_input":"2025-02-27T14:38:20.057178Z","iopub.status.idle":"2025-02-27T14:38:20.061441Z","shell.execute_reply.started":"2025-02-27T14:38:20.057154Z","shell.execute_reply":"2025-02-27T14:38:20.060565Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train_with_influential_samples(model, train_loader, test_loader, selected_samples, tokenizer):\n    \"\"\"\n    Train the model with the original training data plus the selected influential samples.\n    \"\"\"\n    # Create dataset from selected samples\n    selected_df = pd.DataFrame({\n        'SMILES': selected_samples['smiles'],\n        'Label': selected_samples['label']\n    })\n    \n    # Create dataset WITHOUT the idx key\n    class CompatibleExternalDataset(Dataset):\n        def __init__(self, dataframe, tokenizer, max_length=512):\n            self.dataframe = dataframe\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n            \n        def __len__(self):\n            return len(self.dataframe)\n        \n        def __getitem__(self, idx):\n            # Get the SMILES string and label from the DataFrame\n            smiles = self.dataframe.iloc[idx]['SMILES']\n            label = self.dataframe.iloc[idx]['Label']\n            \n            # Tokenize the SMILES string\n            encoding = self.tokenizer(\n                smiles,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            # Remove the batch dimension\n            input_ids = encoding['input_ids'].squeeze(0)\n            attention_mask = encoding['attention_mask'].squeeze(0)\n            \n            # Convert label to tensor\n            label = torch.tensor(label, dtype=torch.float32)\n            \n            # Return without idx key to match original dataset\n            return {\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'labels': label\n            }\n    \n    selected_dataset = CompatibleExternalDataset(selected_df, tokenizer)\n    \n    # Combine original training data with selected samples\n    combined_dataset = torch.utils.data.ConcatDataset([train_loader.dataset, selected_dataset])\n    \n    # Create a new dataloader\n    combined_loader = DataLoader(\n        combined_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    \n    print(f\"Original training set size: {len(train_loader.dataset)}\")\n    print(f\"Number of influential samples added: {len(selected_dataset)}\")\n    print(f\"Combined training set size: {len(combined_dataset)}\")\n    \n    # Train model with combined dataset\n    # Create a fresh model starting from the pre-trained weights\n    fresh_model = MoLFormerWithRegressionHead(base_model).to(DEVICE)\n    fresh_model.load_state_dict(torch.load(\"new_best_model.pth\", map_location=DEVICE))\n    \n    # Train the model\n    print(\"Training model with influential samples...\")\n    enhanced_model, _ = train_model(fresh_model, combined_loader, test_loader)\n    \n    # Evaluate the enhanced model\n    print(\"Evaluating enhanced model...\")\n    enhanced_metrics = evaluate_model(enhanced_model, test_loader)\n    \n    return enhanced_model, enhanced_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:38:26.096519Z","iopub.execute_input":"2025-02-27T14:38:26.096800Z","iopub.status.idle":"2025-02-27T14:38:26.104175Z","shell.execute_reply.started":"2025-02-27T14:38:26.096779Z","shell.execute_reply":"2025-02-27T14:38:26.103494Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main function to run the influence-based data selection workflow.\n    \"\"\"\n    # Step 1: Compute influence scores for the external dataset\n    print(\"Computing influence scores for external dataset...\")\n    influence_df = compute_external_influence_scores(\n        model, external_loader, train_loader, test_loader\n    )\n    \n    # Save influence scores\n    influence_df.to_csv(\"influence_scores.csv\", index=False)\n    print(\"Influence scores saved to influence_scores.csv\")\n    \n    # Visualize influence score distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist(influence_df['influence_score'], bins=30, alpha=0.7)\n    plt.axvline(x=0, color='red', linestyle='--')\n    plt.title(\"Distribution of Influence Scores for External Samples\")\n    plt.xlabel(\"Influence Score\")\n    plt.ylabel(\"Count\")\n    plt.grid(alpha=0.3)\n    plt.savefig(\"influence_distribution.png\")\n    plt.show()\n    \n    # Step 2: Select top influential samples\n    print(\"Selecting top influential samples...\")\n    selected = select_influential_samples(influence_df, top_k=100)\n    \n    # Step 3: Train model with selected samples\n    print(\"Training model with selected influential samples...\")\n    enhanced_model, enhanced_metrics = train_with_influential_samples(\n        model, train_loader, test_loader, selected, tokenizer\n    )\n    \n    # Step 4: Compare performance\n    print(\"\\nPerformance Comparison:\")\n    print(\"-\" * 50)\n    print(\"Baseline Model Metrics:\")\n    for metric, value in baseline_metrics.items():\n        print(f\"  {metric}: {value:.4f}\")\n    \n    print(\"\\nEnhanced Model Metrics:\")\n    for metric, value in enhanced_metrics.items():\n        print(f\"  {metric}: {value:.4f}\")\n    \n    # Compute improvement percentages\n    improvements = {}\n    for metric in baseline_metrics:\n        if metric in ['loss', 'mse', 'rmse', 'mae']:\n            # Lower is better for these metrics\n            delta = baseline_metrics[metric] - enhanced_metrics[metric]\n            pct = (delta / baseline_metrics[metric]) * 100\n            improvements[metric] = pct\n        else:\n            # Higher is better for R²\n            delta = enhanced_metrics[metric] - baseline_metrics[metric]\n            pct = (delta / abs(baseline_metrics[metric])) * 100\n            improvements[metric] = pct\n    \n    print(\"\\nImprovements:\")\n    for metric, pct in improvements.items():\n        print(f\"  {metric}: {pct:.2f}%\")\n    \n    # Save the enhanced model\n    torch.save(enhanced_model.state_dict(), \"enhanced_model.pth\")\n    print(\"\\nEnhanced model saved to enhanced_model.pth\")\n\n# 6. Execute the main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:38:36.084932Z","iopub.execute_input":"2025-02-27T14:38:36.085233Z","iopub.status.idle":"2025-02-27T15:02:50.411987Z","shell.execute_reply.started":"2025-02-27T14:38:36.085208Z","shell.execute_reply":"2025-02-27T15:02:50.411152Z"}},"outputs":[{"name":"stdout","text":"Computing influence scores for external dataset...\nUsing 50 test samples for influence computation\nComputing influence scores...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c266a472f15f4bd28993559cddf19d35"}},"metadata":{}},{"name":"stdout","text":"Influence scores saved to influence_scores.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Selecting top influential samples...\nSelected 100 samples with influence scores ranging from 0.0477 to 0.1125\nTraining model with selected influential samples...\nOriginal training set size: 3360\nNumber of influential samples added: 100\nCombined training set size: 3460\nTraining model with influential samples...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-19-460bd3c05962>:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  fresh_model.load_state_dict(torch.load(\"new_best_model.pth\", map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784e6e2877df496ca1360458ae28bdce"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Training Loss: 0.2309, Validation Loss: 0.4718\nNew best model saved with validation loss: 0.4718\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c603b1755e14868a4f0b7380a6795fe"}},"metadata":{}},{"name":"stdout","text":"Epoch 2, Training Loss: 0.2124, Validation Loss: 0.4619\nNew best model saved with validation loss: 0.4619\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a57df9374674f199b697d66e6601c72"}},"metadata":{}},{"name":"stdout","text":"Epoch 3, Training Loss: 0.1815, Validation Loss: 0.4689\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e5ded3e9684b0caca520c3cbc73b37"}},"metadata":{}},{"name":"stdout","text":"Epoch 4, Training Loss: 0.1722, Validation Loss: 0.4356\nNew best model saved with validation loss: 0.4356\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb171eb1d6914c7e91ac6fb35253535d"}},"metadata":{}},{"name":"stdout","text":"Epoch 5, Training Loss: 0.1537, Validation Loss: 0.4679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b51cfa797824d0596381cbce69d6d8d"}},"metadata":{}},{"name":"stdout","text":"Epoch 6, Training Loss: 0.1401, Validation Loss: 0.4431\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"140e293da33a43ba93a386ca99345119"}},"metadata":{}},{"name":"stdout","text":"Epoch 7, Training Loss: 0.1271, Validation Loss: 0.4608\nEarly stopping triggered after 7 epochs\nEvaluating enhanced model...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-0d9e3a482ed7>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"new_best_model.pth\", map_location=DEVICE))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating model:   0%|          | 0/53 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065bd0ef38754495bdad8ff4e629b08a"}},"metadata":{}},{"name":"stdout","text":"\nTest Results:\nAverage Loss: 0.4414\nMSE: 0.4424\nRMSE: 0.6651\nMAE: 0.5097\nR²: 0.7223\n\nPerformance Comparison:\n--------------------------------------------------\nBaseline Model Metrics:\n  loss: 0.4706\n  mse: 0.4735\n  rmse: 0.6881\n  mae: 0.5384\n  r2: 0.7028\n\nEnhanced Model Metrics:\n  loss: 0.4414\n  mse: 0.4424\n  rmse: 0.6651\n  mae: 0.5097\n  r2: 0.7223\n\nImprovements:\n  loss: 6.20%\n  mse: 6.57%\n  rmse: 3.34%\n  mae: 5.34%\n  r2: 2.78%\n\nEnhanced model saved to enhanced_model.pth\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}